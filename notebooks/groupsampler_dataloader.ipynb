{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a36ae8-3cd8-4c93-8075-7dfa890041ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as image \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.dataset import *\n",
    "import utils.dataset\n",
    "# from train import PapsClsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5593c2-2aca-4558-b112-509ccdf97aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/68782144/pytorch-can-i-group-batches-by-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11f812e-ba0a-4156-8b41-634e6186df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../lbp_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b275caf-b17e-468a-806d-d707b2ee424f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5998, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba3b5b0-a3c5-43ab-a441-d30e4fbbfcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5664, 14)\n"
     ]
    }
   ],
   "source": [
    "dataset = PapsDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83bb211a-6b2c-4ccd-a876-e9db88328621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using [0, 100, 140, 200, 300, 400] as bins for aspect ratio quantization\n",
      "Count of instances per bin: [ 655 1745 2087  966  211]\n"
     ]
    }
   ],
   "source": [
    "from utils.sampler_by_group import *\n",
    "group_ids = create_area_groups(dataset, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5273fed-5117-4067-a9cb-c345653b5700",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "    #test_sampler = torch.utils.data.distributed.DistributedSampler(dataset_test)\n",
    "else:\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    #test_sampler = torch.utils.data.SequentialSampler(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ef72a3-810d-43f6-b258-b9bd068b8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "#list(train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c417c5f-b689-4b16-80f4-baa7bb1971ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, batch_size)\n",
    "#list(train_batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ecd1c-338e-4478-a3e1-681e71f97e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8697ce07-d4ae-46c8-af89-e347172f5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.collate import collate_fn\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_sampler=train_batch_sampler, num_workers=4, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c32930-2503-4b83-a09c-8106d49adc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "torch.Size([8, 3, 221, 204])\n",
      "tensor([2, 1, 3, 0, 1, 3, 3, 1], dtype=torch.uint8)\n",
      "tensor([179, 155, 177, 159, 197, 143, 179, 184], dtype=torch.int32)\n",
      "torch.Size([8, 3, 255, 180])\n",
      "tensor([0, 3, 1, 0, 0, 3, 3, 3], dtype=torch.uint8)\n",
      "tensor([154, 171, 161, 186, 159, 145, 159, 168], dtype=torch.int32)\n",
      "torch.Size([8, 3, 153, 134])\n",
      "tensor([3, 1, 3, 1, 0, 2, 0, 0], dtype=torch.uint8)\n",
      "tensor([136, 129, 120, 125, 133, 131, 124, 126], dtype=torch.int32)\n",
      "torch.Size([8, 3, 309, 280])\n",
      "tensor([0, 0, 1, 1, 3, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([223, 224, 215, 206, 227, 294, 216, 226], dtype=torch.int32)\n",
      "torch.Size([8, 3, 211, 217])\n",
      "tensor([0, 0, 3, 3, 0, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([166, 193, 159, 152, 199, 149, 172, 165], dtype=torch.int32)\n",
      "torch.Size([8, 3, 162, 159])\n",
      "tensor([3, 0, 0, 0, 2, 3, 3, 1], dtype=torch.uint8)\n",
      "tensor([106, 123, 130, 129, 101, 114, 100, 138], dtype=torch.int32)\n",
      "torch.Size([8, 3, 131, 154])\n",
      "tensor([0, 1, 2, 0, 0, 3, 0, 1], dtype=torch.uint8)\n",
      "tensor([119, 119, 107, 119, 107, 113, 116, 101], dtype=torch.int32)\n",
      "torch.Size([8, 3, 179, 200])\n",
      "tensor([0, 0, 1, 1, 3, 0, 0, 3], dtype=torch.uint8)\n",
      "tensor([149, 147, 168, 160, 144, 176, 142, 149], dtype=torch.int32)\n",
      "torch.Size([8, 3, 149, 120])\n",
      "tensor([0, 0, 3, 0, 2, 0, 0, 3], dtype=torch.uint8)\n",
      "tensor([80, 95, 97, 92, 85, 85, 94, 95], dtype=torch.int32)\n",
      "torch.Size([8, 3, 189, 136])\n",
      "tensor([3, 0, 3, 3, 2, 1, 1, 3], dtype=torch.uint8)\n",
      "tensor([122, 121, 106, 138, 130, 136, 123, 127], dtype=torch.int32)\n",
      "*****************************************\n",
      "torch.Size([8, 3, 167, 145])\n",
      "tensor([0, 1, 1, 0, 3, 3, 1, 0], dtype=torch.uint8)\n",
      "tensor([106, 120, 133, 119, 107, 133, 121, 105], dtype=torch.int32)\n",
      "torch.Size([8, 3, 196, 283])\n",
      "tensor([0, 0, 1, 0, 3, 3, 3, 1], dtype=torch.uint8)\n",
      "tensor([163, 180, 168, 165, 183, 199, 154, 143], dtype=torch.int32)\n",
      "torch.Size([8, 3, 164, 146])\n",
      "tensor([2, 3, 0, 1, 1, 0, 1, 3], dtype=torch.uint8)\n",
      "tensor([126, 139, 132, 137, 134, 126, 136, 103], dtype=torch.int32)\n",
      "torch.Size([8, 3, 448, 448])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([342, 344, 492, 353, 383, 394, 358, 421], dtype=torch.int32)\n",
      "torch.Size([8, 3, 194, 226])\n",
      "tensor([0, 3, 0, 1, 0, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([151, 150, 146, 160, 192, 186, 190, 167], dtype=torch.int32)\n",
      "torch.Size([8, 3, 159, 191])\n",
      "tensor([1, 0, 1, 3, 2, 0, 3, 0], dtype=torch.uint8)\n",
      "tensor([132, 117, 135, 100, 128, 139, 118, 106], dtype=torch.int32)\n",
      "torch.Size([8, 3, 116, 142])\n",
      "tensor([0, 0, 3, 0, 0, 0, 2, 1], dtype=torch.uint8)\n",
      "tensor([72, 97, 93, 71, 96, 61, 85, 98], dtype=torch.int32)\n",
      "torch.Size([8, 3, 183, 206])\n",
      "tensor([1, 2, 0, 0, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([143, 148, 190, 151, 174, 160, 149, 152], dtype=torch.int32)\n",
      "torch.Size([8, 3, 206, 235])\n",
      "tensor([0, 1, 3, 1, 3, 3, 1, 1], dtype=torch.uint8)\n",
      "tensor([176, 156, 179, 173, 176, 184, 184, 168], dtype=torch.int32)\n",
      "torch.Size([8, 3, 170, 165])\n",
      "tensor([1, 2, 0, 1, 3, 0, 3, 2], dtype=torch.uint8)\n",
      "tensor([123, 117, 118, 120, 134, 115, 105, 104], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2) :\n",
    "    print('*****************************************')\n",
    "    for i, targets in enumerate(data_loader) :\n",
    "        images, labels, areas = targets\n",
    "        if i < 10 :\n",
    "            print(images.shape)\n",
    "            print(labels)\n",
    "            print(areas)\n",
    "        else :\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a2af323-f3e1-44bf-9eb2-fe109565c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_sampler=train_batch_sampler, num_workers=4, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79792511-317e-4fbd-8052-add157243a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    # batch_size=self.batch_size,\n",
    "    # shuffle=True,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f10cf7-5e30-48c7-ae56-9687332d3cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea0a187c-2d5a-4ba4-94b4-7b12cfdbee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 28\n",
      "29 36\n",
      "31 30\n",
      "18 39\n",
      "20 47\n"
     ]
    }
   ],
   "source": [
    "batch_img = []\n",
    "x = np.random.randint(10,50, 5)\n",
    "y = np.random.randint(10,50, 5)\n",
    "\n",
    "for i, j in zip(x, y) :\n",
    "    print(i, j)\n",
    "    img = np.random.randn(i,j,3)\n",
    "    batch_img.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf3a17bf-5276-40a1-9794-4de59bc128ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 28, 3)\n",
      "(29, 36, 3)\n",
      "(31, 30, 3)\n",
      "(18, 39, 3)\n",
      "(20, 47, 3)\n"
     ]
    }
   ],
   "source": [
    "for b in batch_img :\n",
    "    print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce1d8134-19c2-4b18-b0a8-049a52bb7030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 47 18 28\n"
     ]
    }
   ],
   "source": [
    "max_x = max([ b.shape[0] for b in batch_img])\n",
    "max_y = max([ b.shape[1] for b in batch_img])\n",
    "min_x = min([ b.shape[0] for b in batch_img])\n",
    "min_y = min([ b.shape[1] for b in batch_img])\n",
    "print(max_x, max_y, min_x, min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c159ac-e61e-4eaf-acd6-74d7f1f48112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89a0e6e3-98b8-4b92-b572-6c1b9e2cb982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = np.pad(image, ((pl_x,pr_x), (pl_y,pr_y), (0,0)), 'constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39148c7a-62da-4930-833a-d35e8e4323f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 28, 3)\n",
      "(29, 36, 3)\n",
      "(31, 30, 3)\n",
      "(18, 39, 3)\n",
      "(20, 47, 3)\n"
     ]
    }
   ],
   "source": [
    "for b in batch_img :\n",
    "    print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46ccfedd-8211-4c53-bb10-f538e9dd30ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 47, 3)\n",
      "(45, 47, 3)\n",
      "(45, 47, 3)\n",
      "(45, 47, 3)\n",
      "(45, 47, 3)\n"
     ]
    }
   ],
   "source": [
    "p_image = ([np.pad(x, ((0, max_x - x.shape[0]), (0, max_y - x.shape[1]), (0,0))) for x in batch_img])\n",
    "for p in p_image :\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d29b0377-8e5b-4653-b92b-97c447d0f107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 45, 47, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_image = np.stack([np.pad(x, ((0, max_x - x.shape[0]), (0, max_y - x.shape[1]), (0,0))) for x in batch_img])\n",
    "s_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3bb92-aa23-4681-91ad-9031a693f9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b11074-5cd5-460f-9208-32dee1bcc56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75ec65-c2b4-40c5-8e69-f265c242d1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8fbdbc-be6d-4e28-a931-fc3056f3b200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
